# Minimal sweep configuration for quick testing
# Focuses on a few key hyperparameters
program: train.py
method: grid
metric:
  name: val/loss/overall
  goal: minimize

parameters:
  # Dataset parameters - small fixed values for quick iteration
  graph_d:
    values: [5000]
  graph_l:
    value: 5
  graph_vocab_size:
    value: '1max'
  graph_holdout_percentage:
    value: 0.2
  num_pause_tokens:
    values: [4]
  use_undirected:
    value: true
  use_directional_tokens:
    value: false
  use_task_tokens:
    value: false
  
  # Model architecture parameters - sweep these
  n_layer:
    values: [9]
  n_head:
    values: [8]
  n_embd:
    values: [64]
  dropout:
    values: [0.01]
  bias:
    value: false
  
  # Training hyperparameters - sweep learning rate
  learning_rate:
    value: 1e-3
  label_smoothing:
    value: 0.1
  weight_decay:
    value: 0.01
  beta1:
    value: 0.9
  beta2:
    value: 0.95
  grad_clip:
    value: 1.0
  
  # Learning rate schedule
  decay_lr:
    value: true
  warmup_frac:
    value: 0.10
  lr_decay_frac:
    value: 0.80
  min_lr:
    value: 1e-6
  
  # Fixed training parameters
  epochs:
    value: 1000  # Shorter for quick testing
  edge_iterations_per_epoch: 
    value: 100 # 8k edges total 
  path_iterations_per_epoch: 
    value: 100  # 200 paths in validation, 800 paths in training 
  gradient_accumulation_steps:
    value: 9  # Reduced from 4 for fewer optimizer steps, better GPU utilization
  compile:
    value: true
  eval_interval:
    value: 10  # Increased from 50 to reduce evaluation overhead
  log_interval:
    value: 1  # Increased from 10 to reduce print overhead

  wandb_project:
    value: pathstar_interleave

