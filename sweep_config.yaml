program: train.py
method: bayes
metric:
  name: val/loss/overall
  goal: minimize

parameters:
  # Dataset parameters
  graph_d:
    values: [50, 100, 250, 500]
  graph_l:
    values: [5, 7, 9, 11]
  graph_vocab_size:
    value: 2000
  graph_holdout_percentage:
    values: [0.1, 0.2, 0.3]
  num_pause_tokens:
    values: [1, 3, 5]
  use_undirected:
    value: true
  use_directional_tokens:
    value: true
  
  # Model architecture parameters
  n_layer:
    values: [2, 3, 4, 6]
  n_head:
    values: [4, 8, 12]
  n_embd:
    values: [64, 96, 128, 192]
  dropout:
    values: [0.0, 0.1, 0.2]
  bias:
    values: [false, true]
  
  # Training hyperparameters
  learning_rate:
    distribution: log_uniform_values
    min: 1e-4
    max: 1e-2
  label_smoothing:
    values: [0.0, 0.05, 0.1, 0.15]
  weight_decay:
    values: [0.0, 0.001, 0.01, 0.1]
  beta1:
    value: 0.9
  beta2:
    values: [0.95, 0.98, 0.99]
  grad_clip:
    values: [0.5, 1.0, 2.0]
  
  # Learning rate schedule
  decay_lr:
    value: true
  warmup_frac:
    values: [0.05, 0.10, 0.15]
  lr_decay_frac:
    values: [0.95, 0.99, 1.0]
  min_lr:
    distribution: log_uniform_values
    min: 1e-6
    max: 1e-4
  
  # Fixed training parameters
  epochs:
    value: 50000
  gradient_accumulation_steps:
    value: 1
  compile:
    value: true
  eval_interval:
    value: 100
  log_interval:
    value: 10

