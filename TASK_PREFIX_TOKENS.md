# Task Prefix Tokens for InWeightsPathStar

## Overview

The `InWeightsPathStar` class now includes **task prefix tokens** (`<PATH>` and `<EDGE>`) that are prepended to every sequence. These tokens enable models to distinguish between path prediction and edge memorization tasks, which is critical for effective multi-task learning when using interleaved datasets.

## Motivation

When training on interleaved datasets that mix path prediction and edge memorization sequences, the model needs a clear signal to identify which task it should perform. Without task prefixes, the model would have to infer the task from the sequence structure alone, which can be ambiguous and lead to poor performance.

### Problem Without Task Prefixes

**Ambiguous sequences:**
```
[leaf, PAUSE, root, n_2, n_3, n_4, n_5]  # Is this a path or something else?
[x, y, PAUSE, PAUSE, PAUSE, PAUSE, PAUSE]  # Is this an edge or a truncated path?
```

The model must guess the task based on sequence length and content, which is unreliable.

### Solution With Task Prefixes

**Clear task identification:**
```
[<PATH>, leaf, PAUSE, root, n_2, n_3, n_4, n_5]  # Clearly a path prediction task
[<EDGE>, x, y, PAUSE, PAUSE, PAUSE, PAUSE, PAUSE]  # Clearly an edge memorization task
```

The first token explicitly indicates the task, enabling the model to condition its behavior accordingly.

## Token Definitions

### Token Values

The task prefix tokens are defined relative to the pause token:

```python
self.TASK_TOKENS = {
    'PATH': self.pause_token + 1,
    'EDGE': self.pause_token + 2,
}
```

**Example with d=5, l=5:**
- Vertices: 0-20 (21 vertices total)
- Pause token: 21
- PATH token: 22
- EDGE token: 23

### Vocabulary Size

The effective vocabulary size increases by 2 to accommodate the task tokens:

```python
# Without task tokens:
vocab_size = len(vertices) + 1  # vertices + PAUSE

# With task tokens:
vocab_size = len(vertices) + 3  # vertices + PAUSE + PATH + EDGE
```

## Sequence Formats

### Path Prediction Sequences

**Format:**
```
[<PATH>, leaf, <PAUSE>, <PAUSE>, ..., <PAUSE>, root, n_2, n_3, ..., n_ℓ]
```

**Length:** `l + 2 + num_pause_tokens`

**Example (d=5, l=5, num_pause_tokens=1):**
```
[22, 16, 21, 0, 13, 14, 15, 16]
 ↑   ↑   ↑   ↑  ↑   ↑   ↑   ↑
PATH leaf PAUSE root n2 n3 n4 leaf
```

### Edge Memorization Sequences

**Format:**
```
[<EDGE>, x, y, <PAUSE>, <PAUSE>, ..., <PAUSE>]
```

**Length:** `3` (padded to match path sequence length)

**Example (padded to length 8):**
```
[23, 5, 9, 21, 21, 21, 21, 21]
 ↑   ↑  ↑  ↑   ↑   ↑   ↑   ↑
EDGE x  y  PAUSE (padding)
```

## Usage

### Automatic Application

Task prefix tokens are **automatically added** to all sequences generated by:

1. `generate_path_prediction_training_set()` - Adds `<PATH>` prefix
2. `_generate_interleaved_dataset()` - Adds `<PATH>` to paths, `<EDGE>` to edges

**No manual intervention required!**

### Accessing Token Values

```python
from pathstar import InWeightsPathStar

generator = InWeightsPathStar(d=5, l=5)

# Access task tokens
path_token = generator.TASK_TOKENS['PATH']
edge_token = generator.TASK_TOKENS['EDGE']

print(f"PATH token: {path_token}")  # e.g., 22
print(f"EDGE token: {edge_token}")  # e.g., 23
```

### Checking Sequence Task Type

```python
import torch

# Generate interleaved dataset
generator = InWeightsPathStar(d=5, l=5)
sequences = generator._generate_interleaved_dataset(
    size=100,
    num_pause_tokens=1,
    ratio_path=3,
    ratio_edge=1
)

# Check task type of first sequence
first_token = sequences[0, 0].item()
if first_token == generator.TASK_TOKENS['PATH']:
    print("This is a path prediction sequence")
elif first_token == generator.TASK_TOKENS['EDGE']:
    print("This is an edge memorization sequence")
```

## Metadata

Task tokens are saved in the metadata file for later reference:

```python
import pickle

with open('./data/inweights_pathstar_d5_l5/meta.pkl', 'rb') as f:
    meta = pickle.load(f)

# Access task tokens
task_tokens = meta['task_tokens']
print(task_tokens)  # {'PATH': 22, 'EDGE': 23}

# Access string representations
path_str = meta['itos'][task_tokens['PATH']]
edge_str = meta['itos'][task_tokens['EDGE']]
print(f"{path_str} = {task_tokens['PATH']}")  # <PATH> = 22
print(f"{edge_str} = {task_tokens['EDGE']}")  # <EDGE> = 23

# Reverse lookup
path_token_id = meta['stoi']['<PATH>']
edge_token_id = meta['stoi']['<EDGE>']
```

## Model Implementation Considerations

### Input Embedding

The model's embedding layer must accommodate the task tokens:

```python
import torch.nn as nn

# Get vocab size from metadata
vocab_size = meta['vocab_size']  # Includes task tokens

# Create embedding layer
embedding = nn.Embedding(vocab_size, embed_dim)
```

### Task-Conditional Behavior

Models can use the task prefix to condition their behavior:

```python
def forward(self, x):
    # x shape: [batch_size, seq_len]
    
    # Extract task prefix (first token)
    task_tokens = x[:, 0]  # [batch_size]
    
    # Embed all tokens
    embeddings = self.embedding(x)  # [batch_size, seq_len, embed_dim]
    
    # Option 1: Use task token as conditioning signal
    task_embedding = embeddings[:, 0, :]  # [batch_size, embed_dim]
    
    # Option 2: Check task type and route to task-specific heads
    is_path = (task_tokens == self.path_token_id)
    is_edge = (task_tokens == self.edge_token_id)
    
    # Process accordingly...
```

### Attention Masking

When computing attention, you may want to treat the task prefix specially:

```python
# Allow all tokens to attend to the task prefix
# This gives the model access to task information at every position
task_prefix_mask = torch.zeros(batch_size, seq_len, 1)
task_prefix_mask[:, :, 0] = 1  # All positions can attend to position 0
```

## Benefits

### 1. Clear Task Identification

The model immediately knows which task to perform from the first token, eliminating ambiguity.

### 2. Improved Multi-Task Learning

Task prefixes enable better multi-task learning by providing explicit task signals:
- Shared representations can be learned for common patterns
- Task-specific behaviors can be conditioned on the prefix
- Reduces interference between tasks

### 3. Easier Evaluation

You can easily filter sequences by task during evaluation:

```python
# Separate path and edge sequences
path_mask = (sequences[:, 0] == generator.TASK_TOKENS['PATH'])
edge_mask = (sequences[:, 0] == generator.TASK_TOKENS['EDGE'])

path_sequences = sequences[path_mask]
edge_sequences = sequences[edge_mask]

# Evaluate each task separately
path_accuracy = evaluate_paths(model, path_sequences)
edge_accuracy = evaluate_edges(model, edge_sequences)
```

### 4. Flexible Task Mixing

You can easily create datasets with different task ratios and the model will handle them correctly:

```python
# 90% paths, 10% edges
generator.prepare(train_size=100000, interleave_ratio=(9, 1))

# 50% paths, 50% edges
generator.prepare(train_size=100000, interleave_ratio=(1, 1))

# Model handles both configurations thanks to task prefixes
```

## Backward Compatibility

### Pure Path Prediction

Even when not using interleaving, path sequences include the `<PATH>` prefix:

```python
# Pure path prediction (no interleaving)
generator = InWeightsPathStar(d=5, l=5)
sequences = generator.generate_path_prediction_training_set(size=1000)

# All sequences start with <PATH> token
assert (sequences[:, 0] == generator.TASK_TOKENS['PATH']).all()
```

This ensures consistency across all datasets, whether interleaved or not.

### Edge Memorization Only

If you only want edge memorization (not currently supported as a standalone method), you would need to:

```python
# Generate edges
x, y = generator.generate_edge_memorization_training_set(size=1000)

# Manually add EDGE prefix
edge_token = torch.full((1000,), generator.TASK_TOKENS['EDGE'], dtype=torch.long)
sequences = torch.stack([edge_token, x, y], dim=1)
```

## Examples

### Example 1: Inspecting Generated Sequences

```python
from pathstar import InWeightsPathStar

generator = InWeightsPathStar(d=3, l=4)

# Generate interleaved dataset
sequences = generator._generate_interleaved_dataset(
    size=10,
    num_pause_tokens=1,
    ratio_path=1,
    ratio_edge=1
)

print(f"Sequence shape: {sequences.shape}")
print(f"PATH token: {generator.TASK_TOKENS['PATH']}")
print(f"EDGE token: {generator.TASK_TOKENS['EDGE']}")
print(f"PAUSE token: {generator.pause_token}")

# Inspect first few sequences
for i in range(5):
    seq = sequences[i]
    task_token = seq[0].item()
    if task_token == generator.TASK_TOKENS['PATH']:
        print(f"Sequence {i}: PATH - {seq.tolist()}")
    elif task_token == generator.TASK_TOKENS['EDGE']:
        print(f"Sequence {i}: EDGE - {seq.tolist()}")
```

### Example 2: Task-Specific Loss Computation

```python
def compute_loss(model, sequences, targets):
    """Compute task-specific losses"""
    
    # Get task tokens
    task_tokens = sequences[:, 0]
    
    # Separate by task
    path_mask = (task_tokens == generator.TASK_TOKENS['PATH'])
    edge_mask = (task_tokens == generator.TASK_TOKENS['EDGE'])
    
    # Compute path loss
    if path_mask.any():
        path_loss = F.cross_entropy(
            model(sequences[path_mask]),
            targets[path_mask]
        )
    else:
        path_loss = 0.0
    
    # Compute edge loss
    if edge_mask.any():
        edge_loss = F.cross_entropy(
            model(sequences[edge_mask]),
            targets[edge_mask]
        )
    else:
        edge_loss = 0.0
    
    # Combined loss (can weight differently)
    total_loss = path_loss + edge_loss
    
    return total_loss, path_loss, edge_loss
```

### Example 3: Curriculum Learning with Task Prefixes

```python
# Phase 1: Edge-only training
generator.prepare(train_size=50000, interleave_ratio=(0, 1))
# All sequences start with <EDGE>

# Phase 2: Mostly edges, some paths
generator.prepare(train_size=50000, interleave_ratio=(1, 9))
# 10% <PATH>, 90% <EDGE>

# Phase 3: Balanced
generator.prepare(train_size=50000, interleave_ratio=(1, 1))
# 50% <PATH>, 50% <EDGE>

# Phase 4: Path-focused
generator.prepare(train_size=50000, interleave_ratio=(9, 1))
# 90% <PATH>, 10% <EDGE>
```

## Comparison: With vs Without Task Prefixes

| Aspect | Without Prefixes | With Prefixes |
|--------|------------------|---------------|
| Task Identification | Implicit (from structure) | Explicit (first token) |
| Model Complexity | Must infer task | Direct task signal |
| Multi-Task Learning | Difficult, high interference | Easier, clear separation |
| Sequence Length | l + 1 + num_pause (path) | l + 2 + num_pause (path) |
| Vocabulary Size | vertices + 1 | vertices + 3 |
| Evaluation | Must separate by heuristics | Easy filtering by prefix |
| Flexibility | Limited | High (easy task mixing) |

## Future Enhancements

Potential extensions to the task prefix system:

1. **More Task Types**: Add prefixes for other tasks (e.g., `<SHORTEST_PATH>`, `<CYCLE_DETECTION>`)
2. **Task Difficulty Levels**: Add prefixes like `<EASY>`, `<MEDIUM>`, `<HARD>`
3. **Compositional Tasks**: Combine multiple prefixes (e.g., `<PATH>`, `<REVERSE>`)
4. **Task Embeddings**: Learn task-specific embeddings instead of discrete tokens
5. **Dynamic Task Selection**: Allow model to choose which task to perform

## Summary

Task prefix tokens (`<PATH>` and `<EDGE>`) are a simple but powerful addition that:

✅ Enable clear task identification  
✅ Improve multi-task learning  
✅ Simplify model implementation  
✅ Facilitate task-specific evaluation  
✅ Support flexible curriculum learning  
✅ Maintain backward compatibility  

They are automatically applied to all sequences and require no manual intervention from users. The tokens are saved in metadata for reproducibility and can be easily accessed for model implementation.

